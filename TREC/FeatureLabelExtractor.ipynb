{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, os, collections, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import collections\n",
    "import preprocessor as p\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## annotators are just the wiki information about the events\n",
    "## events - there are 6 events in total - each has the following\n",
    "#\n",
    "#\n",
    "#\n",
    "# \n",
    "# events \n",
    "#     |_event1\n",
    "#     |      |_ eventid\n",
    "#     |      |_  tweets   \n",
    "#     |             |_[{tweet_1: id, category et al}, {tweet_2}, {tweet_2},......{tweet_n}]\n",
    "#     |\n",
    "#     |_ event2\n",
    "#     |       |_ eventid\n",
    "#     |       |_  tweets   \n",
    "#     |             |_[{tweet_1: id, category et al}, {tweet_2}, {tweet_2},......{tweet_n}]\n",
    "#\n",
    "#\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filen = 'TRECIS-2018-TestEvents-Labels/'\n",
    "files = os.listdir(filen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getevents(filename):\n",
    "    with open(filename, 'r') as fp:\n",
    "        content = json.loads(fp.read())\n",
    "    allevents = []\n",
    "    for eachevent in range(len(content['events'])):\n",
    "        allevents.append(content['events'][eachevent]['eventid'])\n",
    "    return allevents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gettweets(filename):\n",
    "    with open(filename, 'r') as fp:\n",
    "        content = json.loads(fp.read())\n",
    "    alltwts = []\n",
    "    allcats = []\n",
    "    alltopics = []\n",
    "    for eachevent in range(len(content['events'])):\n",
    "        val = len(content['events'][eachevent]['tweets'])\n",
    "        topics = [content['events'][eachevent]['eventid'] for _ in range(val)]\n",
    "        twts = [content['events'][eachevent]['tweets'][eachtweet]['postID'] for eachtweet in range(val)]\n",
    "        cats = [content['events'][eachevent]['tweets'][eachtweet]['categories'] for eachtweet in range(val)]\n",
    "        alltwts.extend(twts)\n",
    "        allcats.extend(cats)\n",
    "        alltopics.extend(topics)\n",
    "    return alltwts, allcats, alltopics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = []\n",
    "theirids = []\n",
    "theirtopics = []\n",
    "for each in files:\n",
    "    left, right, middle = gettweets(filen+each)\n",
    "    categories.extend(right)\n",
    "    theirids.extend(left)\n",
    "    theirtopics.extend(middle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19784, 19784, 19784)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(categories), len(theirids), len(theirtopics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeldf = pd.DataFrame([theirids, theirtopics, categories]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeldf.columns = ['tweetids', 'alltopics', 'categories']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get methods for features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'datasets/'\n",
    "files = os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(name, files):\n",
    "    event = [eve for eve in files if name in eve]\n",
    "    print(\"Files found:\", event)\n",
    "    tweetids = []\n",
    "    alltweets = []\n",
    "    alltopics = []\n",
    "    for index in event:\n",
    "        with open('datasets_json/'+index , 'r') as fp:\n",
    "            data = fp.readlines()\n",
    "            ids = [json.loads(each)['allProperties']['id'] for each in data]\n",
    "            twts = [json.loads(each)['allProperties']['text'] for each in data]\n",
    "            topics = [json.loads(each)['topic'] for each in data]\n",
    "        tweetids.extend(ids)\n",
    "        alltweets.extend(twts)\n",
    "        alltopics.extend(topics)\n",
    "    return tweetids, alltweets, alltopics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = [each.split('.')[1] for each in files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_names =    ['Earthquake','fire', 'typhoon', 'Floods', 'Tornado', 'Bombing', 'Shooting', 'Attacks']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Event features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(name, df2):\n",
    "    tweetids, alltweets, alltopics = get_dataset(name, files)\n",
    "    # create a dataframe for the tweetids\n",
    "    df = pd.DataFrame([tweetids, alltopics, alltweets]).T\n",
    "    # labels the columns - needed for meging with labels\n",
    "    df.columns = ['tweetids', 'topics', 'tweets']\n",
    "    print(\"Number of tweet ids found: \",len(set(tweetids)))\n",
    "    # dataset where only tweetids are there is merged with labels dataset\n",
    "    dfn = pd.merge(df, df2, on = 'tweetids').drop_duplicates(subset = ['tweetids'])\n",
    "    # during merge there may not be all tweets available\n",
    "    print(\"Number of records seen after merging with labels:\",len(dfn))\n",
    "    # get the count of intersection of dataset with tweetids and label dataset so as to verify the merge step.\n",
    "    print(\"Actual number of common ids seen: \",len(set(df['tweetids'].tolist()).intersection(set(df2['tweetids'].tolist()))))\n",
    "    # check the not foudn tweetids\n",
    "    nf =  [each for each in tweetids if not each in dfn['tweetids'].tolist()]\n",
    "    #\n",
    "    print(\"Not found tweets count:\",len(nf))\n",
    "    # get the categories list from the merged dataframe\n",
    "    allcategories = dfn['categories'].tolist()\n",
    "    # unlisting list of lists. so as to get the unique number of labels\n",
    "    allabels = list(itertools.chain(*allcategories))\n",
    "    # these are the unqie number of labels with their respective count in the dataframe\n",
    "    labels = list(collections.Counter(allabels).keys())\n",
    "    # creating a hashmap for the labels to list - one hot encoding\n",
    "    labelmap = collections.defaultdict(list)\n",
    "    # first fill each label key with 0 vector of size = dfn lenght\n",
    "    for each in labels:\n",
    "        labelmap[each] = [0]*len(dfn)\n",
    "    # then for each in row cateorin in the dfn, whicever labels are presentin that row, assign 1\n",
    "    for row in range(len(dfn)):\n",
    "        for col in dfn.iloc[row]['categories']:\n",
    "            labelmap[col][row] = 1\n",
    "    # replace the each label col with the hashmap we created\n",
    "    for key, val in labelmap.items():\n",
    "        dfn[key] = val\n",
    "    \n",
    "    # before writing to file. clean the tweets\n",
    "    tocleantweets = dfn['tweets'].tolist()\n",
    "    # defininf the what to be removed from the tweets\n",
    "    p.set_options(p.OPT.URL, p.OPT.EMOJI, p.OPT.RESERVED,  p.OPT.MENTION)\n",
    "    # cleaned tweets after preprocessing \n",
    "    cleantweets = [p.clean(data) for data in tocleantweets]\n",
    "    # wrting back to the dataframe\n",
    "    dfn['cleanedtweets'] = cleantweets\n",
    "    # drop the uncleaned tweets \n",
    "    dfn.drop('tweets', axis=1, inplace= True)\n",
    "    # save as the csv file\n",
    "    dfn.to_csv('Features/'+name+'_features.csv')\n",
    "    print(\"File save as: {}_features.csv\".format(name))\n",
    "    print(\"-----------------------------------------------------------------------------\")\n",
    "    \n",
    "    return \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files found: ['trecis2019-A-test.earthquakeCalifornia2014.json']\n",
      "Number of tweet ids found:  128\n",
      "Number of records seen after merging with labels: 0\n",
      "Actual number of common ids seen:  0\n",
      "Not found tweets count: 128\n",
      "File save as: earthquakeCalifornia2014_features.csv\n",
      "-----------------------------------------------------------------------------\n",
      "Files found: ['trecis2019-A-test.floodChoco2019.json']\n",
      "Number of tweet ids found:  674\n",
      "Number of records seen after merging with labels: 0\n",
      "Actual number of common ids seen:  0\n",
      "Not found tweets count: 854\n",
      "File save as: floodChoco2019_features.csv\n",
      "-----------------------------------------------------------------------------\n",
      "Files found: ['trecis2019-A-test.hurricaneFlorence2018.json']\n",
      "Number of tweet ids found:  2500\n",
      "Number of records seen after merging with labels: 0\n",
      "Actual number of common ids seen:  0\n",
      "Not found tweets count: 2500\n",
      "File save as: hurricaneFlorence2018_features.csv\n",
      "-----------------------------------------------------------------------------\n",
      "Files found: ['trecis2019-A-test.earthquakeBohol2013.json']\n",
      "Number of tweet ids found:  646\n",
      "Number of records seen after merging with labels: 0\n",
      "Actual number of common ids seen:  0\n",
      "Not found tweets count: 646\n",
      "File save as: earthquakeBohol2013_features.csv\n",
      "-----------------------------------------------------------------------------\n",
      "Files found: ['trecis2019-A-test.fireAndover2019.json']\n",
      "Number of tweet ids found:  360\n",
      "Number of records seen after merging with labels: 0\n",
      "Actual number of common ids seen:  0\n",
      "Not found tweets count: 375\n",
      "File save as: fireAndover2019_features.csv\n",
      "-----------------------------------------------------------------------------\n",
      "Files found: ['trecis2019-A-test.fireYMM2016.json']\n",
      "Number of tweet ids found:  2500\n",
      "Number of records seen after merging with labels: 0\n",
      "Actual number of common ids seen:  0\n",
      "Not found tweets count: 2500\n",
      "File save as: fireYMM2016_features.csv\n",
      "-----------------------------------------------------------------------------\n",
      "Files found: ['trecis2019-A-test.shootingDallas2017.json']\n",
      "Number of tweet ids found:  2500\n",
      "Number of records seen after merging with labels: 0\n",
      "Actual number of common ids seen:  0\n",
      "Not found tweets count: 2500\n",
      "File save as: shootingDallas2017_features.csv\n",
      "-----------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for each in events:\n",
    "    get_features(each, labeldf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
